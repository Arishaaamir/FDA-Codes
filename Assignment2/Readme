# 📊 Financial Data Analytics — Assignment 2

This repository contains the code and analysis for **Assignment 2** of the Financial Data Analytics course. It focuses on fraud detection in insurance claims using machine learning techniques, including logistic regression, data preprocessing, class balancing, and performance evaluation.

---

## 📂 Contents

* `FDA assignment 2.py`: Main Python script containing data loading, preprocessing, model training, evaluation, and visualization.
* **Dataset** (external reference): `insurance_claims.csv` (not included in the repository — update the file path as needed).

---

## 🔍 Features & Workflow

✅ **Data Preprocessing**

* Loads insurance claims dataset
* Drops irrelevant or redundant columns
* Applies one-hot encoding for categorical variables

✅ **Class Balancing**

* Uses SMOTE (Synthetic Minority Oversampling Technique) to handle class imbalance in fraud detection

✅ **Model Training**

* Implements a Logistic Regression classifier with balanced class weights
* Standardizes numerical features for consistent scaling

✅ **Model Evaluation**

* Computes accuracy, classification report (precision, recall, F1-score)
* Displays a confusion matrix heatmap
* Plots ROC curve and computes AUC-ROC for binary classification performance

✅ **Visualization**

* Correlation heatmap
* Confusion matrix heatmap
* ROC curve plot

✅ **Insights**

* Notes on accuracy and interpretation of evaluation metrics

---

## ⚙️ How to Run

1. **Install Required Libraries**
   Make sure you have Python 3.x installed, then install dependencies:

```bash
pip install pandas numpy matplotlib seaborn plotly scikit-learn imbalanced-learn yellowbrick
```

2. **Prepare Dataset**
   Ensure the `insurance_claims.csv` file is placed in the correct directory or update the file path in the script.

3. **Run Script**
   Execute the Python file:

```bash
python FDA\ assignment\ 2.py
```

---

## 📈 Output & Interpretation

* **Accuracy Score** → Measures overall correctness
* **Classification Report** → Shows precision, recall, F1-score
* **Confusion Matrix** → Visual overview of true vs. predicted classifications
* **ROC Curve & AUC** → Evaluates classifier performance across thresholds

Note: High accuracy alone is not sufficient — review precision, recall, and AUC carefully, especially in imbalanced datasets.

